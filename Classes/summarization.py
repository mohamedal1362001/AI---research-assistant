# -*- coding: utf-8 -*-
"""summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e57BDOV_AYhmW2eifpDB0Wk0WhC0c1nY
"""

import nltk
import torch
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords 
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from transformers.pipelines.base import abstractmethod
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from pytorch_pretrained_bert import BertTokenizer, BertModel
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer
#----------------------------------
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
stop_words = set(stopwords.words('english'))
def clean_text(text):
    #Remove http links 
    text = re.sub(r'http\S+', '',text, flags=re.MULTILINE)
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# instantiate

#---------------------------------------------------------------------------------
def Bert_Load():
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  bert_model = BertModel.from_pretrained('bert-base-uncased')
  return tokenizer,bert_model

def T5_Load(T5_PATH):
  t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)
  t5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH)
  return t5_tokenizer,t5_model


def bertSent_embeding(sentences ,tokenizer ,bert_model):
   
    ## Add sentence head and tail as BERT requested
    marked_sent = ["[CLS] " +item + " [SEP]" for item in sentences]
    
    tokenized_sent = [tokenizer.tokenize(item ) for item in marked_sent]
    

    
    ## index to BERT vocabulary
    indexed_tokens = [tokenizer.convert_tokens_to_ids(item) for item in tokenized_sent]
    tokens_tensor = [torch.tensor([item]) for item in indexed_tokens]

    ## add segment id as BERT requested
    segments_ids = [[1] * len(item) for ind,item in enumerate(tokenized_sent)]
    segments_tensors = [torch.tensor([item]) for item in segments_ids]
    ## load BERT base model and set to evaluation mode
    ##### bert_model = SentenceTransformer('stsb-bert-large')


    #model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. 
    #For example, Dropouts Layers, BatchNorm Layers etc.
    #You need to turn them off during model evaluation, and .eval() will do it for you. In addition, 
    #the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:
    bert_model.eval()
    
    ## Output 12 layers of latent vector
    assert len(tokens_tensor) == len(segments_tensors)
    encoded_layers_list = []
    for i in range(len(tokens_tensor)):
        with torch.no_grad():
            encoded_layers, _ = bert_model(tokens_tensor[i], segments_tensors[i])
        encoded_layers_list.append(encoded_layers)
    
    ## Use only the last layer vetcor **we can use others**
  
    token_vecs_list =[layers[11][0] for layers in encoded_layers_list]
    ## Pooling word vector to sentence vector, use mean pooling
    sentence_embedding_list = [torch.mean(vec, dim=0).numpy() for vec in token_vecs_list]

    return sentence_embedding_list

def kmeans_sumIndex(sentence_embedding_list):
  
    try:
      n_clusters =5
      #np.ceil(len(sentence_embedding_list)**0.5)
      kmeans = KMeans(n_clusters=int(n_clusters))
      kmeans = kmeans.fit(sentence_embedding_list)
      #Compute minimum distances between one point and a set of points.
      #get centers and compare with sentence_embedding by bert !!!!("centers is the most common sentences")
      sum_index,_ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentence_embedding_list,metric='euclidean')
      sum_index = sorted(sum_index)
      return sum_index
    except:
      n_clusters =1
      #np.ceil(len(sentence_embedding_list)**0.5)
      kmeans = KMeans(n_clusters=int(n_clusters))
      kmeans = kmeans.fit(sentence_embedding_list)
      #Compute minimum distances between one point and a set of points.
      #get centers and compare with sentence_embedding by bert !!!!("centers is the most common sentences")
      sum_index,_ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentence_embedding_list,metric='euclidean')
      sum_index = sorted(sum_index)
      return sum_index



def bertSummarize(text,  Bert_Tokenizer, Bert_Model):
    
    sentences = sent_tokenize(text)

    sentence_embedding_list = bertSent_embeding(sentences, Bert_Tokenizer, Bert_Model)

    sum_index = kmeans_sumIndex(sentence_embedding_list)
    
    summary = ' '.join([sentences[ind] for ind in sum_index])
   
    return summary


#----------make Extractive  summary  on  Abstract going from resnet101 ------
def Extractive_Summarization(ListOfAbstracts,  Bert_Tokenizer, Bert_Model):
  Extractive_summary = []
  print("ListOfAbstracts: ",ListOfAbstracts)
  if type(ListOfAbstracts)==list  :
    ListOfAbstracts = [item for item in ListOfAbstracts if item is not None]
    for abstract in  ListOfAbstracts :
      abstract = [item for item in abstract if item is not None]
      sum =bertSummarize(''.join(abstract),  Bert_Tokenizer, Bert_Model) #instead of ''.join
      print("sum",type(sum))
      Extractive_summary.append(sum)
      print("Extractive_summary",Extractive_summary)
  else:
      Extractive_summary = (bertSummarize(ListOfAbstracts,  Bert_Tokenizer, Bert_Model))

  return Extractive_summary


#----------make Abstractive summary  on  Multi , Single docs and from text ------
def Abstractive_Summarization(Extractive_summary, T5_Tokenizer,T5_Model ):

  if isinstance(Extractive_summary, list):
    i=0
    out_text=[]
    text =' '.join(Extractive_summary)
    while(i<len(text)):
      inputs = T5_Tokenizer.encode(text[i:i+512], return_tensors="pt", max_length=512, padding='max_length')
      summary_ids = T5_Model.generate(inputs, num_beams=int(2), no_repeat_ngram_size=3, length_penalty=2.0,min_length=50, max_length=100, early_stopping=True)
      Last_Summary = T5_Tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
      out_text.append(Last_Summary)
      i+=512
    Last_Summary = ''.join(out_text)

    '''inputs = T5_Tokenizer.encode(text, return_tensors="pt", max_length=512, padding='max_length')
    summary_ids = T5_Model.generate(inputs, num_beams=int(2), no_repeat_ngram_size=3, length_penalty=2.0,min_length=50, max_length=100, early_stopping=True)
    Last_Summary = T5_Tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
'''
  else:
    i=0
    out_text=[]
    while(i<len(Extractive_summary)):
      inputs = T5_Tokenizer.encode(Extractive_summary[i:i+512], return_tensors="pt", max_length=512, padding='max_length')
      summary_ids = T5_Model.generate(inputs, num_beams=int(2), no_repeat_ngram_size=3, length_penalty=2.0,min_length=50, max_length=100, early_stopping=True)
      Last_Summary = T5_Tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
      out_text.append(Last_Summary)
      i+=512
    Last_Summary = ''.join(out_text)
'''
    inputs = T5_Tokenizer.encode(Extractive_summary, return_tensors="pt", max_length=512, padding='max_length')
    summary_ids = T5_Model.generate(inputs, num_beams=int(2), no_repeat_ngram_size=3, length_penalty=2.0,min_length=50, max_length=100, early_stopping=True)
    Last_Summary = T5_Tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
'''
  return Last_Summary

#----------Last  point  to  run the  functions-------------------
def Do_Summarization(ListOfAbstracts, Bert_Tokenizer, Bert_Model, T5_Tokenizer,T5_Model):
  Sumarry_1 = Extractive_Summarization(ListOfAbstracts, Bert_Tokenizer, Bert_Model)
  ABS_Summary = Abstractive_Summarization(Sumarry_1,T5_Tokenizer ,T5_Model)
  return ABS_Summary



