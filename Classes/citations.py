# -*- coding: utf-8 -*-
"""citations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kwFKgOAyFK8hCH0JZS8URdInKhVvOi4w

# Citation Generation NoteBook

---

*   APA
*   MLA
*   Chigaco

---

**Models used in this notebook**
# > PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x

   

*   PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x is a deep learning model for document layout analysis. It is based on the Mask R-CNN object detection framework and is trained on the PubLayNet dataset, which is a large dataset of document images.

*   PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x can be used to detect and segment different types of document layout elements, such as tables, text blocks, and images. It can also be used to extract text from documents.

*   The model is available for free on the Layout Parser website. It can be used with the Layout Parser Python library to detect and segment document layout elements in images.


*   Here are some of the key features of PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x:

  * It is based on the Mask R-CNN object detection framework.
  * It is trained on the PubLayNet dataset, which is a large dataset of document images.
  * It can be used to detect and segment different types of document layout elements, such as tables, text blocks, and images.
  * It can also be used to extract text from documents.



*   Here are some of the limitations of PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x:

  * It is a deep learning model, so it requires a lot of memory and processing power.
  * It is not always accurate, especially for documents that are not well-aligned or have low quality.
  * It can be difficult to interpret the results of the model, such as the segmentation masks and extracted text.

# > en_core_web_sm

*   en_core_web_sm is a small English language model from the spaCy library that is trained on a massive dataset of text and code. It can be used for a variety of tasks, including:

  * Named entity recognition (NER)
  * Part-of-speech tagging (POS tagging)
  * Dependency parsing
  * Semantic similarity
  * Text generation

*   en_core_web_sm is a smaller model than en_core_web_trf, so it requires less memory and processing power. It is also more accurate for rare or uncommon words. However, it is not as powerful as en_core_web_trf for some tasks, such as semantic similarity and text generation.


*   Here are some of the key features of en_core_web_sm:

  * It is trained on a massive dataset of text and code.
  * It is based on the Transformer architecture.
  * It can be used for a variety of tasks, including NER, POS tagging, dependency parsing, semantic similarity, and text generation.
  * It is smaller than en_core_web_trf, so it requires less memory and processing power.
  * It is more accurate for rare or uncommon words.

*  Here are some of the limitations of en_core_web_sm:

  * It is not as powerful as en_core_web_trf for some tasks, such as semantic similarity and text generation.

  
# > en_core_web_trf



*   en_core_web_trf is a large language model (LLM) from the spaCy library that is trained on a massive dataset of text and code. It can be used for a variety of tasks, including:

  *  Named entity recognition (NER)
  * Part-of-speech tagging (POS tagging)
  * Dependency parsing
  * Semantic similarity
  * Text generation
*   en_core_web_trf is based on the Transformer architecture, which is a neural network architecture that has been shown to be very effective for natural language processing tasks. It is trained on a massive dataset of text and code, which allows it to learn the statistical relationships between words and phrases. en_core_web_trf is a powerful tool that can be used for a variety of tasks. It is easy to use and can be integrated into a variety of applications.


*   Here are some of the key features of en_core_web_trf:

  * It is trained on a massive dataset of text and code.
  * It is based on the Transformer architecture.
  * It can be used for a variety of tasks, including NER, POS tagging, dependency parsing, semantic similarity, and text generation.
  * It is easy to use and can be integrated into a variety of applications.

*  Here are some of the limitations of en_core_web_trf:

  * It is a large model, so it requires a lot of memory and processing power.
  * It is not always accurate, especially for rare or uncommon words.
  * It can be difficult to interpret the results of some of the tasks that it can perform, such as semantic similarity and text generation.

# Install Dependencies

# Import The Libraries
"""

import re
import os
import cv2
import spacy
import string
import shutil
import spacy.cli
import numpy as np
import pytesseract
import pandas as pd
import torch.nn as nn
import en_core_web_trf
import pybtex.database
import spacy_transformers
import layoutparser as lp
import traceback
import multiprocessing
from tqdm import tqdm
import pdf2image
import pdfplumber
from PIL import Image
# print(pdfplumber.__version__)
from docx import Document


"""#  The 'pdf_Splitter' Function

---

This function is responsible for dividing the file into images
And save it in a path and send the path to the next function

"""
Author= ''
Department=''
Date=''
Title=''

def Models_Load(Config,Model):
  nlp_trf = spacy.load("en_core_web_trf")
  nlp_trf = en_core_web_trf.load()
  model = lp.Detectron2LayoutModel(config_path = Config, model_path = Model,label_map={0: "Abstract", 1: "Author", 2: "Caption", 3:"Date", 4:"Equation",5: "Figure", 6: "Footer", 7: "List", 8:"Paragraph", 9:"Reference", 10: "Section", 11: "Table", 12:"Title"})
  return nlp_trf,model

def pdf_Splitter(pdf_file, data_dir):

    try:
        pdf_images = pdf2image.convert_from_path(os.path.join(data_dir, pdf_file))
    except Exception as e:
      print("Error during PDF conversion:")
      print(traceback.format_exc())
      return
    
    try:
        pdf = pdfplumber.open(os.path.join(data_dir, pdf_file))
    except Exception as e:
      print("Error occurred while opening the PDF file:")
      print(traceback.format_exc())  
      return
    
    for page_id in tqdm(range(len(pdf.pages))):

        img_out='/content/pdf_img'
        if not os.path.exists(img_out):
            os.makedirs(img_out)

        img_path= (img_out+'/'+pdf_file.replace('.pdf', '') + '_{}_ori.jpg'.format(str(page_id)))
        pdf_images[page_id].save(img_path)

    return img_out #give me  the images dirc
    
def delete_pdfs_from_directory(directory):
    for filename in os.listdir(directory):
        if filename.endswith(".pdf"):
            file_path = os.path.join(directory, filename)
            os.remove(file_path)
            
def delete_img_from_directory(directory):
    if os.path.exists(directory):
    # Directory exists, so delete it
      shutil.rmtree(directory)
    else:
      print("Directory does not exist.")
    
   
  
"""# The '**preprocess**' Function 


---
This function is responsible for extracting the words from the image using OCR and sending the image and words as an output

"""

def preprocess(image_path):

    image = Image.open(image_path)
    image = image.convert("RGB")

    width, height = image.size
    w_scale = 1000 / width
    h_scale = 1000 / height
    ocr_df = pytesseract.image_to_data(image, output_type='data.frame')#check done
    ocr_df = ocr_df.dropna().assign(left_scaled=ocr_df.left * w_scale,
                    width_scaled=ocr_df.width * w_scale,
                    top_scaled=ocr_df.top * h_scale,
                    height_scaled=ocr_df.height * h_scale,
                    right_scaled=lambda x: x.left_scaled + x.width_scaled,
                    bottom_scaled=lambda x: x.top_scaled + x.height_scaled)
    float_cols = ocr_df.select_dtypes('float').columns
    ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)
    ocr_df = ocr_df.replace(r'^\s*$!', np.nan, regex=True)
    ocr_df = ocr_df.dropna().reset_index(drop=True)
    words = list(ocr_df.text)
    return image, words

"""# The '**get_title_of_paper**' Function


---
This function uses the first image to extract the title from it only by '**mask_rcnn_X_101_32x8d_FPN_3x** ', and then sends it to the next function

"""

def extract_informations(dir_image,model):
  image = cv2.imread(dir_image)
  image = image[..., ::-1] 
  #********************************************************
  layout = model.detect(image) 
  lp.draw_box(image, layout)
  #************************************************************
  title_blocks = lp.Layout([b for b in layout if b.type=="Title"])
  author_blocks = lp.Layout([b for b in layout if b.type=="Author"])
  date_blocks = lp.Layout([b for b in layout if b.type=="Date"])
  #paragraph_block = lp.Layout([b for b in layout if b.type=="Paragraph"])
  
  #abstract_blocks = lp.Layout([b for b in layout if b.type=="Abstract"])

  return title_blocks, author_blocks, date_blocks, image

def extract_Abs(dir_image,model):
  image = cv2.imread(dir_image)
  image = image[..., ::-1] 
  #********************************************************
  layout = model.detect(image) 
  lp.draw_box(image, layout)
  abstract_blocks = lp.Layout([b for b in layout if b.type=="Abstract"])

  return abstract_blocks ,image

def Decode_text(TextBlock ,image):
  text_blocks = lp.Layout([b for b in TextBlock])
  #*************************************************************
  h, w = image.shape[:2]
  left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)
  left_blocks = text_blocks.filter_by(left_interval, center=True)
  left_blocks.sort(key = lambda b:b.coordinates[1])

  right_blocks = [b for b in text_blocks if b not in left_blocks]
  right_blocks.sort(key = lambda b:b.coordinates[1])
  # And finally combine the two list and add the index
  # according to the order
  text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])
  ocr_agent = lp.TesseractAgent(languages='eng') 
  for block in text_blocks:
    segment_image = (block
                       .pad(left=5, right=5, top=5, bottom=5)
                       .crop_image(image))
      
    text = ocr_agent.detect(segment_image)
    block.set(text=text, inplace=True)
    Decoded_Text=text_blocks.get_texts()

    return Decoded_Text

def extract_author_name(doc):
  authors_names=[]
  for ent in doc.ents:
    if ent.label_ == 'PERSON':
      authors_names.append(ent.text)
  return authors_names

def extract_author_department(doc):
  authors_department=[]
  for ent in doc.ents:
    if ent.label_ == 'ORG':
      authors_department.append(ent.text)
  return authors_department

def extract_paper_date(doc):
  paper_date=[]
  for ent in doc.ents:
    if ent.label_ == 'DATE':
      paper_date.append(ent.text)
  return paper_date

"""# Extract Title, Auther ,Date ,Department 


---


 To make the Citation
"""

def Authers_and_Departments(AutherFromX101, paragraph,TrfModel):

  authorDepartment=[]
  #paperDate=[]
  doc_trf = TrfModel(AutherFromX101)
  AuthorName = extract_author_name(doc_trf)
  authorDepartment = extract_author_department(doc_trf)
  if len(AuthorName) == 0:
    doc_trf = TrfModel(paragraph)
    AuthorName = extract_author_name(doc_trf)
    
  if len(authorDepartment)==0 :
    doc_trf = TrfModel(paragraph)
    authorDepartment = extract_author_department(doc_trf)
     
  return AuthorName,authorDepartment

"""# Citation Format Section


---


"""

def make_citation_APA_format(authorName,authorDepartment,paperDate,paperTitle):
  final_citation=''

  if type(authorName)==list:                       
    for name in authorName:
      final_citation=final_citation + name +","
    final_citation=final_citation + "."
  else:
     final_citation=final_citation + authorName +"."


  if type(paperDate)==list:
    for date in paperDate:
      final_citation = final_citation +"("+ date +")" +","
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + paperDate +"."

  if type(paperTitle)==list:
    for title in paperTitle:
      final_citation=final_citation + title +","  
    final_citation=final_citation + "."
  else:
    final_citation=final_citation + paperTitle + "."

  if type(authorDepartment)==list :
    for department in authorDepartment:
      final_citation=final_citation + department +", "
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + authorDepartment +"."
    
  return final_citation

def make_citation_MLA_format(authorName,authorDepartment,paperDate,paperTitle):
  final_citation=''
  if type(authorName)==list:                       
    for name in authorName:
      final_citation=final_citation + name +","
    final_citation=final_citation + "."
  else:
     final_citation=final_citation + authorName +"."

  if type(paperTitle)==list:
    for title in paperTitle:
      final_citation=final_citation + title +","  
    final_citation=final_citation + "."
  else:
    final_citation=final_citation + paperTitle + "."

  if type(authorDepartment)==list :
    for department in authorDepartment:
      final_citation=final_citation + department +", "
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + authorDepartment +"."

  if type(paperDate)==list:
    for date in paperDate:
      final_citation = final_citation + date +","
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + paperDate +"."

  
    


  return final_citation

def make_citation_Chicago_format(authorName,authorDepartment,paperDate,paperTitle):
  final_citation=''
  if type(authorName)==list:                       
    for name in authorName:
      final_citation=final_citation + name +","
    final_citation=final_citation + "."
  else:
     final_citation=final_citation + authorName +"."


  if type(paperDate)==list:
    for date in paperDate:
      final_citation = final_citation + date  +","
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + paperDate +"."

  if type(paperTitle)==list:
    for title in paperTitle:
      final_citation=final_citation + title +","  
    final_citation=final_citation + "."
  else:
    final_citation=final_citation + paperTitle + "."

  if type(authorDepartment)==list :
    for department in authorDepartment:
      final_citation=final_citation + department +", "
    final_citation=final_citation + "."
  else:
      final_citation=final_citation + authorDepartment +"."
   
  return final_citation

"""# Test Function to  test all citation format"""
def department_finder(imagdir , abstract):
  image, words= preprocess(imagdir)
  newwords= ''
  first_word = abstract.split(' ', 1)[0]
  for word in words :
    if word == first_word:
      break
    else:
      newwords = newwords + ' ' + word
  return newwords
    
    
    
  
  
def GenerateCHICAGOCitation(directory ,model101,modelTRF ):

      citaion_Chicago = ''
      if os.path.exists(directory):
          file_list = os.listdir(directory)
          for filename in file_list:     
              if filename.endswith('_0_ori.jpg') or filename.endswith('_0_ori.jpeg'):
                  image_path = os.path.join(directory, filename)
                  
                  title, author, date,image =  extract_informations(image_path,model101)
                  abstract = Extract_Abstract(directory,model101)
                  #------------------------------------
                  Title = Decode_text(title , image)
                  Title = ' '.join([a if a is not None else '' for a in Title])
                  
                  TotalAuthor = Decode_text(author , image)
                  TotalAuthor = ' '.join([a if a is not None else '' for a in TotalAuthor])

                  Date = Decode_text(date , image)
                  if Date is not None : 
                    Date = ' '.join([a if a is not None else '' for a in Date])
                  else:
                     Date = str(Date)
                  Abstract = ''
                  for abslist in abstract:
                    Abstract = ' '.join([a if a is not None else '' for a in abslist])
                  new_words = department_finder(image_path,Abstract)
                  #------------------------------------
                  Author,Department =Authers_and_Departments(TotalAuthor,new_words,modelTRF)
                  citaion_Chicago = make_citation_Chicago_format(Author,Department,Date,Title)
      else:
          print(f"Directory '{directory}' does not exist.")
      return citaion_Chicago

def GenerateAPACitation(directory ,model101,modelTRF ):
      citaion_APA = ''
      if os.path.exists(directory):
          file_list = os.listdir(directory)
          for filename in file_list:
              
              if filename.endswith('_0_ori.jpg') or filename.endswith('_0_ori.jpeg'):
                  image_path = os.path.join(directory, filename)
                  title, author, date,image =  extract_informations(image_path,model101)
                  abstract = Extract_Abstract(directory,model101)
                  print("abstract from resnet",abstract)
                  #------------------------------------
                  Title = Decode_text(title , image)
                  Title = ' '.join([a if a is not None else '' for a in Title])
                  
                  TotalAuthor = Decode_text(author , image)
                  TotalAuthor = ' '.join([a if a is not None else '' for a in TotalAuthor])

                  Date = Decode_text(date , image)
                  if Date is not None : 
                    Date = ' '.join([a if a is not None else '' for a in Date])
                  else:
                     Date = str(Date)
                  Abstract = ''
                  for abslist in abstract:
                    Abstract = ' '.join([a if a is not None else '' for a in abslist])
                  new_words = department_finder(image_path,Abstract)
                  #------------------------------------
                  Author,Department =Authers_and_Departments(TotalAuthor,new_words,modelTRF)
                  citaion_APA = make_citation_APA_format(Author,Department,Date,Title)
      else:
          print(f"Directory '{directory}' does not exist.")
      return citaion_APA

def GenerateMLACitation(directory ,model101,modelTRF ):

      citaion_MLA = ''
      if os.path.exists(directory):
          file_list = os.listdir(directory)
          for filename in file_list:
              
              if filename.endswith('_0_ori.jpg') or filename.endswith('_0_ori.jpeg'):
                  image_path = os.path.join(directory, filename)
                  title, author, date,image =  extract_informations(image_path,model101)
                  abstract = Extract_Abstract(directory,model101)
                  #------------------------------------
                  Title = Decode_text(title , image)
                  Title = ' '.join([a if a is not None else '' for a in Title])
                  
                  TotalAuthor = Decode_text(author , image)
                  TotalAuthor = ' '.join([a if a is not None else '' for a in TotalAuthor])

                  Date = Decode_text(date , image)
                  if Date is not None : 
                    Date = ' '.join([a if a is not None else '' for a in Date])
                  else:
                     Date = str(Date)
                     
                  Abstract = ''
                  for abslist in abstract:
                    Abstract = ' '.join([a if a is not None else '' for a in abslist])
                  new_words = department_finder(image_path,Abstract)
                  #------------------------------------
                  Author,Department =Authers_and_Departments(TotalAuthor,new_words,modelTRF)
                  citaion_MLA = make_citation_MLA_format(Author,Department,Date,Title)
      else:
          print(f"Directory '{directory}' does not exist.")
      return citaion_MLA

def Extract_Abstract(directory ,model101 ):
  Abstract=[]
  if os.path.exists(directory):
      file_list = os.listdir(directory)
      for filename in file_list:     
          if filename.endswith('_0_ori.jpg') or filename.endswith('_0_ori.jpeg'):
              image_path = os.path.join(directory, filename)
              abstracts,image =  extract_Abs(image_path,model101)
              Abstract.append(Decode_text(abstracts , image))
  else:
      print(f"Directory '{directory}' does not exist.")
  return Abstract

"""# The main Function 

"""

def preprocess_pdf(pdf_data_dir):
  images_path =''
  All_Pdf_citation = []
  pdf_files = list(os.listdir(pdf_data_dir))
  pdf_files = [t for t in pdf_files if t.endswith('.pdf')]
  
  for pdf_file in pdf_files:
    images_path =pdf_Splitter(pdf_file, pdf_data_dir)

  return images_path

def predict_citation(images_path, x101model, trfmodel , Citation_type):
  citation = ''
  if Citation_type == 1:
    citation= GenerateAPACitation(images_path ,x101model,trfmodel)
  elif Citation_type == 2:
    citation= GenerateMLACitation(images_path ,x101model,trfmodel)
  elif Citation_type == 3:
    citation= GenerateCHICAGOCitation(images_path ,x101model,trfmodel)
  return citation

def valid_xml_char_ordinal(c):
    codepoint = ord(c)
    # conditions ordered by presumed frequency
    return (
        0x20 <= codepoint <= 0xD7FF or
        codepoint in (0x9, 0xA, 0xD) or
        0xE000 <= codepoint <= 0xFFFD or
        0x10000 <= codepoint <= 0x10FFFF
        )

"""

> Word Format

"""

def dowload_citation_with_docx_format(citation):
  cleaned_string = ''.join(c for c in citation if valid_xml_char_ordinal(c))
  # Create a new Word document
  document = Document()
  # Add content to the document
  document.add_heading('Reference', level=1)
  document.add_paragraph(cleaned_string)
  # Save the document
  document.save('/content/Reference.docx')

"""

> Bibx Format

"""

def create_file_Bibtex(authorName ,Title,authorDepartment,paperDate):
  #///////////////////////////////////////////////////

  
  

  with open("mycitation.bib", "w") as f:
    f.write("@article{article1,\n")
    f.write("  author = {"+''.join(authorName)+"},\n")
    f.write("  title = {"+Title+",}\n")
    f.write("  department = {"+''.join(authorDepartment) +"},\n")
    f.write("  Date = {"+''.join(paperDate)+"}\n")
    f.write("}")

"""# TEST THE CODE"""